# å¸¸è§é—®é¢˜ (FAQ)

## å®‰è£…å’Œè®¾ç½®

### Q1: ç³»ç»Ÿè¦æ±‚æ˜¯ä»€ä¹ˆï¼Ÿ

**æœ€ä½è¦æ±‚**:
- Python 3.8+
- 8GB RAM
- 10GB å¯ç”¨ç£ç›˜ç©ºé—´
- CPU: 4æ ¸å¿ƒ

**æ¨èé…ç½®**:
- Python 3.10+
- 16GB+ RAM
- 50GB+ SSD
- NVIDIA GPU (å¯é€‰ï¼Œç”¨äºåŠ é€Ÿ)
- 8æ ¸å¿ƒ+ CPU

### Q2: å®‰è£…å¤±è´¥æ€ä¹ˆåŠï¼Ÿ

å°è¯•ä»¥ä¸‹æ­¥éª¤ï¼š

```bash
# 1. å‡çº§pip
pip install --upgrade pip

# 2. å•ç‹¬å®‰è£…å¯èƒ½æœ‰é—®é¢˜çš„åŒ…
pip install torch --index-url https://download.pytorch.org/whl/cpu

# 3. å¦‚æœCUDAç›¸å…³é”™è¯¯ï¼Œå®‰è£…CPUç‰ˆæœ¬
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu

# 4. é‡æ–°å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### Q3: Ollamaå®‰è£…åœ¨å“ªé‡Œï¼Ÿ

- Linux/Mac: å‚è€ƒ https://ollama.com/download
- Windows: ä½¿ç”¨WSL2æˆ–Docker

## ä½¿ç”¨é—®é¢˜

### Q4: æ–‡æ¡£å¯¼å…¥å¾ˆæ…¢æ€ä¹ˆåŠï¼Ÿ

ä¼˜åŒ–æ–¹æ³•ï¼š

```yaml
# åœ¨config.yamlä¸­è°ƒæ•´
embedding:
  batch_size: 64  # å¢å¤§æ‰¹æ¬¡å¤§å°ï¼ˆå¦‚æœå†…å­˜è¶³å¤Ÿï¼‰

document_processing:
  chunk_size: 256  # å‡å°å—å¤§å°ï¼Œç”Ÿæˆæ›´å°‘çš„å—
```

### Q5: æ£€ç´¢ç»“æœä¸ç›¸å…³æ€ä¹ˆåŠï¼Ÿ

1. **è°ƒæ•´Kå€¼**ï¼šå¢åŠ è¿”å›ç»“æœæ•°é‡
```bash
python scripts/run_rag.py --top_k 10
```

2. **æ›´æ¢Embeddingæ¨¡å‹**ï¼šä½¿ç”¨æ›´å¤§çš„æ¨¡å‹
```yaml
embedding:
  model_name: 'sentence-transformers/all-MiniLM-L12-v2'
```

3. **ä¼˜åŒ–åˆ‡å—**ï¼šå‡å°‘chunk_sizeä¿æŒä¸Šä¸‹æ–‡å®Œæ•´æ€§

### Q6: LLMç”Ÿæˆçš„ç­”æ¡ˆè´¨é‡ä¸é«˜ï¼Ÿ

1. **è°ƒæ•´temperature**ï¼š
```bash
python scripts/run_rag.py --temperature 0.3  # æ›´ç¡®å®šæ€§
python scripts/run_rag.py --temperature 0.9  # æ›´æœ‰åˆ›æ„
```

2. **æ›´æ¢æ¨¡å‹**ï¼š
```bash
ollama pull mistral  # æ›´å¼ºå¤§çš„æ¨¡å‹
```

3. **ä¼˜åŒ–æç¤ºè¯**ï¼šä¿®æ”¹config.yamlä¸­çš„prompt_template

### Q7: å¦‚ä½•æ”¯æŒä¸­æ–‡æ–‡æ¡£ï¼Ÿ

ç³»ç»Ÿå·²æ”¯æŒä¸­æ–‡ï¼Œä½†å»ºè®®ï¼š

1. ä½¿ç”¨å¤šè¯­è¨€Embeddingæ¨¡å‹ï¼š
```yaml
embedding:
  model_name: 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'
```

2. ä½¿ç”¨æ”¯æŒä¸­æ–‡çš„LLMï¼š
```bash
ollama pull qwen  # åƒé—®æ¨¡å‹
```

## æ€§èƒ½é—®é¢˜

### Q8: å†…å­˜å ç”¨å¤ªå¤§ï¼Ÿ

**å‡å°‘å†…å­˜å ç”¨**ï¼š

1. å¯ç”¨æ¨¡å‹é‡åŒ–ï¼š
```yaml
llm:
  quantization:
    enabled: true
    bits: 4
```

2. å‡å°batch_sizeï¼š
```yaml
embedding:
  batch_size: 16  # æˆ–æ›´å°
```

3. ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼š
```bash
ollama pull phi  # å¾ˆå°çš„æ¨¡å‹
```

### Q9: å“åº”é€Ÿåº¦æ…¢ï¼Ÿ

**åŠ é€Ÿæ–¹æ³•**ï¼š

1. ä½¿ç”¨GPUï¼ˆå¦‚æœ‰ï¼‰ï¼š
```python
model = EmbeddingModel('model-name', device='cuda')
```

2. å‡å°‘top_kï¼š
```bash
python scripts/run_rag.py --top_k 3
```

3. å¯ç”¨ç¼“å­˜ï¼š
```yaml
performance:
  enable_caching: true
```

### Q10: GPUå†…å­˜ä¸è¶³ï¼Ÿ

ä½¿ç”¨CPUæ¨¡å¼ï¼š
```bash
export CUDA_VISIBLE_DEVICES=""  # ç¦ç”¨GPU
python scripts/run_rag.py
```

æˆ–è€…ä½¿ç”¨é‡åŒ–ï¼š
```yaml
llm:
  quantization:
    enabled: true
    bits: 4  # æå¤§å‡å°‘æ˜¾å­˜å ç”¨
```

## æ•°æ®å’Œå®‰å…¨

### Q11: åŠ å¯†å¯†é’¥ä¸¢å¤±äº†æ€ä¹ˆåŠï¼Ÿ

**æ— æ³•æ¢å¤**ã€‚åŠ å¯†å¯†é’¥ä¸¢å¤±æ„å‘³ç€ï¼š
- æ— æ³•è§£å¯†å·²å­˜å‚¨çš„æ•°æ®
- éœ€è¦é‡æ–°å¯¼å…¥æ‰€æœ‰æ–‡æ¡£

é¢„é˜²æªæ–½ï¼š
```bash
# å¤‡ä»½å¯†é’¥
cp encryption.key encryption.key.backup
```

### Q12: å¦‚ä½•æ›´æ¢åŠ å¯†å¯†é’¥ï¼Ÿ

```bash
# 1. åˆ é™¤æ—§æ•°æ®
rm -rf qdrant_storage/

# 2. ç”Ÿæˆæ–°å¯†é’¥å¹¶é‡æ–°å¯¼å…¥
python scripts/ingest_documents.py \
  --input_dir data/raw/ \
  --generate_key
```

### Q13: æ•°æ®å­˜å‚¨åœ¨å“ªé‡Œï¼Ÿ

```
qdrant_storage/     # å‘é‡æ•°æ®åº“ï¼ˆå«åŠ å¯†æ•°æ®ï¼‰
encryption.key      # åŠ å¯†å¯†é’¥
logs/              # å®¡è®¡æ—¥å¿—
```

## é”™è¯¯å¤„ç†

### Q14: "Collection not found" é”™è¯¯ï¼Ÿ

è¿è¡Œæ–‡æ¡£å¯¼å…¥ï¼š
```bash
python scripts/ingest_documents.py --input_dir data/raw/
```

### Q15: "Model not found" é”™è¯¯ï¼Ÿ

æ‹‰å–æ¨¡å‹ï¼š
```bash
ollama pull llama2
```

### Q16: "Connection refused" é”™è¯¯ï¼Ÿ

å¯åŠ¨OllamaæœåŠ¡ï¼š
```bash
ollama serve
```

### Q17: PDFè§£æå¤±è´¥ï¼Ÿ

ç¡®ä¿å®‰è£…äº†pypdfï¼š
```bash
pip install pypdf
```

å¯¹äºåŠ å¯†çš„PDFï¼Œéœ€è¦å…ˆè§£å¯†ã€‚

## é«˜çº§ç”¨æ³•

### Q18: å¦‚ä½•é›†æˆåˆ°è‡ªå·±çš„åº”ç”¨ï¼Ÿ

```python
from src.rag_pipeline import RAGSystem
# ... åˆå§‹åŒ–ç»„ä»¶
rag = RAGSystem(retriever, llm_client)
result = rag.answer_question("Your question")
```

è¯¦è§ [APIæ–‡æ¡£](API.md)

### Q19: å¦‚ä½•æ‰¹é‡å¤„ç†é—®é¢˜ï¼Ÿ

```python
questions = ["Q1", "Q2", "Q3"]
results = rag_system.batch_answer(questions)
```

### Q20: å¦‚ä½•è‡ªå®šä¹‰æç¤ºè¯ï¼Ÿ

ä¿®æ”¹ config.yamlï¼š
```yaml
rag:
  prompt_template: |
    ä½ çš„è‡ªå®šä¹‰æ¨¡æ¿
    ä¸Šä¸‹æ–‡: {context}
    é—®é¢˜: {question}
    å›ç­”:
```

### Q21: å¦‚ä½•è¯„ä¼°ç³»ç»Ÿæ€§èƒ½ï¼Ÿ

```bash
# è¿è¡ŒåŸºå‡†æµ‹è¯•
python scripts/run_benchmark.py \
  --test_queries data/test_datasets/test_queries.txt \
  --benchmark_type full
```

### Q22: å¦‚ä½•ç›‘æ§ç³»ç»Ÿï¼Ÿ

æŸ¥çœ‹å®¡è®¡æ—¥å¿—ï¼š
```bash
tail -f logs/audit_*.log
```

è·å–ç»Ÿè®¡ä¿¡æ¯ï¼š
```python
from src.audit import AuditLogger
logger = AuditLogger()
stats = logger.get_statistics()
print(stats)
```

## è´¡çŒ®å’Œåé¦ˆ

### Q23: å¦‚ä½•æŠ¥å‘Šbugï¼Ÿ

åœ¨GitHubä¸Šåˆ›å»ºIssueï¼š
https://github.com/Irene-hua/graduation-design/issues

### Q24: å¦‚ä½•è´¡çŒ®ä»£ç ï¼Ÿ

1. Forkä»“åº“
2. åˆ›å»ºfeatureåˆ†æ”¯
3. æäº¤Pull Request

### Q25: åœ¨å“ªé‡Œè·å–æ›´å¤šå¸®åŠ©ï¼Ÿ

- ğŸ“š [README](../README.md)
- ğŸ—ï¸ [æ¶æ„æ–‡æ¡£](ARCHITECTURE.md)
- ğŸ”§ [APIæ–‡æ¡£](API.md)
- ğŸš€ [å¿«é€Ÿå¼€å§‹](../QUICKSTART.md)

---

é—®é¢˜æœªè§£å†³ï¼Ÿ[æäº¤Issue](https://github.com/Irene-hua/graduation-design/issues)
