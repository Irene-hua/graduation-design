# 面向隐私保护的轻量级RAG系统

## 项目简介

本项目是一个面向本地化部署和隐私敏感场景的轻量级检索增强生成（RAG）系统。系统在保证问答准确性的前提下，集成端到端的数据加密机制以保护私有知识库的安全，同时通过采用轻量级深度学习模型和量化技术优化系统的推理延迟和资源占用。

## 核心特性

### 1. 隐私保护机制
- **AES-GCM加密**: 使用256位AES-GCM算法对文档内容进行加密
- **加密存储**: 向量数据库中仅存储加密密文，原文不直接存储
- **本地部署**: 所有数据和计算均在本地进行，杜绝数据外泄风险

### 2. 轻量级模型
- **嵌入模型**: sentence-transformers/all-MiniLM-L6-v2 (仅22MB)
- **LLM集成**: 支持Ollama运行的各种轻量级大模型
- **资源优化**: 支持CPU运行，适合资源受限环境

### 3. 完整的RAG流程
- **文档处理**: 支持PDF、DOCX、TXT、Markdown、HTML等多种格式
- **智能分块**: 自动将文档切分为合适大小的块
- **向量检索**: 使用Qdrant向量数据库进行高效检索
- **上下文生成**: 基于检索结果生成准确答案

### 4. 审计与日志
- **操作审计**: 记录所有系统操作
- **隐私保护**: 日志中不包含敏感数据
- **完整性校验**: 支持日志完整性验证

## 系统架构

```
┌─────────────────────────────────────────────────────────────┐
│                      用户界面 (CLI/API)                       │
└─────────────────────────────────────────────────────────────┘
                              │
┌─────────────────────────────────────────────────────────────┐
│                      RAG系统核心                             │
├─────────────────────────────────────────────────────────────┤
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐     │
│  │ 文档处理模块  │  │ 加密模块      │  │ 审计模块      │     │
│  └──────────────┘  └──────────────┘  └──────────────┘     │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐     │
│  │ 嵌入模型      │  │ 向量数据库    │  │ LLM客户端    │     │
│  └──────────────┘  └──────────────┘  └──────────────┘     │
└─────────────────────────────────────────────────────────────┘
                              │
┌─────────────────────────────────────────────────────────────┐
│                   本地存储和服务                              │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐     │
│  │ 加密密钥      │  │ Qdrant数据库  │  │ Ollama服务   │     │
│  └──────────────┘  └──────────────┘  └──────────────┘     │
└─────────────────────────────────────────────────────────────┘
```

## 快速开始

### 1. 环境要求

- Python 3.8+
- Ollama (用于运行LLM)
- 4GB+ RAM (推荐8GB)

### 2. 安装依赖

```bash
# 克隆仓库
git clone https://github.com/Irene-hua/graduation-design.git
cd graduation-design

# 创建虚拟环境
python -m venv venv
source venv/bin/activate  # Linux/Mac
# 或 venv\Scripts\activate  # Windows

# 安装依赖
pip install -r requirements.txt
```

### 3. 安装Ollama

访问 [Ollama官网](https://ollama.ai) 下载并安装Ollama。

```bash
# 安装后，启动Ollama服务
ollama serve

# 下载模型（在另一个终端）
ollama pull llama3.2:3b
```

### 4. 配置系统

配置文件位于 `config/config.yaml`，可以根据需要调整：

- 加密算法和密钥配置
- 向量数据库设置
- 嵌入模型选择
- LLM参数配置

### 5. 使用示例

#### 检查系统状态

```bash
python main.py check
```

#### 导入文档

```bash
python main.py ingest --file data/documents/example.pdf
```

#### 查询系统

```bash
python main.py query --question "文档的主要内容是什么？"
```

#### 交互式模式

```bash
python main.py interactive
```

#### 查看集合信息

```bash
python main.py info
```

## 技术细节

### 加密流程

1. **文档处理**: 文档被解析并切分成多个块
2. **向量生成**: 每个文本块生成对应的向量表示
3. **文本加密**: 使用AES-GCM加密原始文本
4. **存储**: 向量和密文一起存储在向量数据库中

### 查询流程

1. **查询向量化**: 用户问题转换为向量
2. **向量检索**: 在向量空间中搜索最相似的文档块
3. **密文解密**: 检索到的密文被解密为原文
4. **答案生成**: LLM基于解密后的上下文生成答案

### 隐私保护措施

1. **数据加密**: 所有敏感数据使用AES-256加密
2. **本地存储**: 密钥和数据仅存储在本地
3. **本地计算**: 所有处理在本地进行
4. **日志脱敏**: 审计日志不包含敏感信息
5. **安全删除**: 临时解密数据使用后立即清除

## 目录结构

```
graduation-design/
├── config/                 # 配置文件
│   └── config.yaml        # 主配置文件
├── data/                  # 数据目录
│   ├── documents/         # 原始文档
│   └── vector_db/         # 向量数据库
├── logs/                  # 日志文件
├── src/                   # 源代码
│   ├── encryption/        # 加密模块
│   ├── retrieval/         # 检索模块
│   ├── generation/        # 生成模块
│   ├── audit/            # 审计模块
│   ├── utils/            # 工具模块
│   ├── ui/               # 用户界面
│   └── rag_system.py     # RAG系统主类
├── tests/                # 测试代码
├── main.py               # 主程序入口
├── requirements.txt      # 依赖列表
└── README.md            # 项目说明
```

## 性能指标

### 模型大小
- 嵌入模型: ~22MB (all-MiniLM-L6-v2)
- LLM: 取决于所选模型 (7B模型约4GB)

### 推理速度
- 文档编码: ~100 chunks/秒 (CPU)
- 向量检索: <100ms
- 答案生成: 1-5秒 (取决于模型和硬件)

### 资源占用
- 内存: 2-4GB (取决于模型和文档数量)
- 存储: 取决于文档数量

## 安全性分析

### 威胁模型
- ✓ 防止数据泄露到外部服务器
- ✓ 防止未授权访问存储数据
- ✓ 防止日志泄露敏感信息
- ✓ 提供操作审计能力

### 加密强度
- AES-256-GCM (NIST认证)
- 认证加密 (防篡改)
- 随机初始化向量

## 扩展与优化

### 可选优化
1. **模型量化**: 使用bitsandbytes进行4-bit量化
2. **GPU加速**: 配置CUDA支持以提升性能
3. **批处理**: 批量处理文档提高效率
4. **缓存机制**: 缓存常用查询结果

### 自定义配置
- 更换嵌入模型
- 调整块大小和重叠
- 配置不同的LLM模型
- 自定义检索参数

## 常见问题

### Q: Ollama连接失败？
A: 确保Ollama服务正在运行 (`ollama serve`)，并且端口11434未被占用。

### Q: 内存不足？
A: 可以选择更小的模型，或减少批处理大小。

### Q: 查询结果不准确？
A: 尝试调整top_k参数、chunk_size或使用更强大的LLM模型。

## 贡献指南

欢迎提交Issue和Pull Request！

## 许可证

本项目仅用于学术研究和毕业设计。

## 致谢

本项目使用了以下开源项目：
- Sentence Transformers
- Qdrant
- Ollama
- Cryptography
- 以及其他在requirements.txt中列出的项目

## 联系方式

如有问题，请通过GitHub Issues联系。
