# yaml
encryption:
  algorithm: AES-256-GCM
  # 从环境变量读取 base64 表示的 32 字节密钥
  key: "env:RAG_ENC_KEY"
  # 可选参数venv\Scripts\activate
  nonce_size: 12
  tag_size: 16

vector_db:
  backend: qdrant        # 或 "faiss" 等
  qdrant:
    host: "127.0.0.1"
    port: 6333
    # Use the collection name that your local Qdrant instance actually holds.
    # The default collection used by the project (created under data/vector_db/collection)
    # is "private_documents". If your Qdrant database uses a different collection,
    # change this value accordingly.
    collection: "private_documents"
    distance: "Cosine"  # Cosine/Euclid/Dot
    storage_path: "./data/vector_db"  # 本地存储时使用

embedding:
  model: "./data/models/all-MiniLM-L6-v2"
  framework: "sentence_transformers"
  device: "cpu"         # 或 "cuda"
  quantize: false
  batch_size: 32
  # If you want to force fully-local operation, ensure the embedding model
  # is present in your local Hugging Face cache. Alternatively you can set
  # `model` to a local path where the model is stored, e.g.:
  # model: "C:/path/to/local/cache/sentence-transformers_all-MiniLM-L6-v2"
  # To run in offline mode, export these environment variables in PowerShell
  # (temporary for the session):
  # $env:HUGGINGFACE_HUB_OFFLINE = "1"
  # $env:TRANSFORMERS_OFFLINE = "1"
  # $env:HF_DATASETS_OFFLINE = "1"

llm:
  provider: "ollama"
  ollama:
    host: "http://127.0.0.1:11434"   # ollama 服务地址
    model: "llama3.2:3b"
    temperature: 0.1
    max_tokens: 1024
    streaming: false

logging:
  level: "INFO"
  log_dir: "./logs"
